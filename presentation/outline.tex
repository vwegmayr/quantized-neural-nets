\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\begin{document}

\pagenumbering{gobble}

\subsubsection*{Motivation}
    \begin{itemize}
        \item Computational Efficiency (inkl. Horowitz) !! Why do we do this?
        \item Improved Generalization (train with low precision (lp) but keep hp weights and use them during test -> better results) Some power gains during training, no power gains during test.
        \item Energy efficiency during test time if you use lp weights, sacrifice accuracy for power saving
        \item Examples and related concepts: dropout, ReLu (sparse gradient),
        influence of feature discretization, 
    \end{itemize}


\subsubsection*{Binarization Schemes}
    \begin{itemize}
        \item Deterministic vs. Stochastic
    \end{itemize}


\subsubsection*{Gradient Estimation}
    \begin{itemize}
        \item Path Derivative Gradient Estimators (reparametrization, ST,
        SlopeAnnealing)
        \item Score Function based Gradient Estimators (reinforce+many others)
        \item Expectation Backpropagation
    \end{itemize}
    See Gumble Softmax Paper for overview

\subsubsection*{Algorithms Adam + BN}
    \begin{itemize}
        \item General Review
        \item Illustrate their importance for QNNs
        \item Implement algorithm to see how it works without Adam and BN?
    \end{itemize}

\subsubsection*{Full Algorithm}
    \begin{itemize}
        \item Try to visualize, illustrate procedure (highlight similarities and
        differences to conventional NN training and inference)
    \end{itemize}

\subsubsection*{Experiments}
    \begin{itemize}
        \item Describe different NN architectures (DNN, CNN, RNN)
        \item Describe data sets (MNIST, CIFAR, ImageNet, PennTree)
        \item binary vs low precision
    \end{itemize}

\subsubsection*{Results}
    \begin{itemize}
        \item Training results
        \item Performance results (look up older benchmarks to compare loss in
        in accuracy to)
        \item find comparisons from other references
    \end{itemize}

\pagebreak

\section*{Evolution of the QNN paper}

\subsubsection*{Training NN with lp multiplications}
    \begin{itemize}
    \item The inital motivation was to reduce the power consumption of multipliers, which are claimed to be the most power hungry.
    \item No Binarization of anything yet, only lp weights
    \item A common pattern already shows: lp multipliers + hp accumulators. Parameter updates are also hp.
    \item Cost of multiplier $O(precision^2)$ while cost of accumulator $O(precision)$
    \item Problem with fixed point: low dynamic range $\rightarrow$ dynamic fixed point.
    \item Evaluation of maxout on MNIST, CIFAR, SVHN with three formats (floating
    point, fixed point, dynamic fixed point)
    \item Techniques: Maxout, dropout, momentum, weight decay, dyn. fixed point, update prc vs prop prec.
    \item Comparison floating point vs. fixed point ?
    \item Explain demand for sufficient precison of update (compared to Propagations) due to SGD updates
    \item Half-precision has little to no impact (hp fine-tuning?)
    \item Couple of plots showing final test error as function of everything
    \item References to lp network training in the 90ies!
    \item No evaluation of computational gains, only show how robust NNs are to lp muls.
    \end{itemize}

\subsubsection*{Binary Connect: Training DNNs with binary weights during propagations}
    \begin{itemize}
    \item Emphasis shifts from efficiency to generalization properties (ala dropout)
    \item Only binarization of weights, activations not yet binary! Backprop still contains muls!
    \item mention discretization as form of noise which preserves expected value of weights.
    \item Again: need for hp accumulation (cite several studies incl haml limtied
    precision one), Interesting: reference which says brain synapse precision is
    around 6-12 bits
    \item Relations to dropout, dropconnect (!), variational weight noise?
    \item DropConnect: Only expected value of weights needs hp
    \item Another reference about hardware cost of add+mul [22]
    \item Binarization: Deterministic vs. stochastic, Hard sigmoid more efficient!
    \item Algorithm: Biases not binarized!
    \item Tricks: Weight clipping, batch norm, adam, normalized initialization,
    L2-SVM output layer
    \item Inference: Three ways: keep binary weights, use hp weights or ensemble of
    binary networks (sampled)
    \item Compute dropout+DNN Performance numbers for CIFAR + SVHN?
    \item Fig 1: feature coadaption comparison
    \item Ref. [39] about binary DNN: Fixed-Point feedforward deep Neural...retrains network!
    \item 2/3 of all muls are due to forward/backward prop
    \item No demonstration of power saving/efficieny gains
    \end{itemize}

\subsubsection*{Neural Networks with few Multiplications}
    \begin{itemize}
    \item Muls again in foucs (``Multiplier light networks'')
    \item Binarization of weights + quantization (!) of activations
    \item No quantization of BN or ADAM
    \item Convert muls in backprop to bitshifts by pow2 quantization of activations
    \item Ref backprop without muls 1999
    \item First mention of random number generation cost
    \item Ternary connect -1,0,+1, sampling scheme similar to lp reference!
    \item how does hp error signal back prop? how about hp input?
    \item Most muls in weight updates (2MN+3M muls at least)
    \item Mention muls incurred by batch norm! Cost of divison!
    \item Table 2 analysis of total num of muls
    \item Compares only to ordinary SGD (but ok they use the same for the QNN)
    \item QNN converges slower but better in the end fig 1
    \item They dont explain the bit shift operations properly
    \item non-uniform distribution of activations
    \item Explanation of regularization by lp weights - VERY similar to entropy
    SDG ideas! large-basin solution, small description length, Additional reference
    (Neelakatan: Adding gradient noise)
    \end{itemize}

\subsubsection*{Quantized Neural Networks}

    \begin{itemize}
        \item Very lp activations + 6bit gradients (enabling bit-wise ops)
        \item first to actually consider gradient of det./stoch. binary units.
        \item first experiments on RNNs + ImageNet
        \item Extensive references (e.g. network compression for inference)
        \item MAC ops replaced by XNOR and popcount ops
        \item CNNs benefit mainly from activation quantization (large neuron to
        weight ratio)
        \item Binary kernel repetition (worth mentioning?)
        \item Abandon stochastic binarization (only in activations at train time)
        \item NEW: Gradient estimators (clipped straight through) due to binary
        activations.
        \item Shaky argument about independence of binarizations for the
        ``derivation'' of the clipped ST estimator. But intuitively ok (eq. 6)
        \item Address cost of BN, which is particularly large for CNNs + inverse sqrt
        \item How does the approximate shift work in case of negative argument?
        \item Make screenshpt of ``very unoptimized'' baseline gpu kernel (theano repo),
        shift based BN+ADAM are in Torch repo
        \item Algorithm 1: input binarization? activation functions missing - no,
        they use det./stoch. binary units!
        \item Need to disentangle back-prop equations...
        \item Input handling: 8bit fixed point , check algorithm 4...
        \item More than one bit: Quantization+XNORpopcount, Ref to DoReFa net and
        logarithmic data representation papers.
    \end{itemize}

\end{document}