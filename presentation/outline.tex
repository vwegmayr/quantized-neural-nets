\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\begin{document}

\pagenumbering{gobble}

\subsubsection*{Motivation}
    \begin{itemize}
        \item Computational Efficiency (inkl. Horowitz) !! Why do we do this?
        \item Improved Generalization (train with low precision (lp) but keep hp weights and use them during test -> better results) Some power gains during training, no power gains during test.
        \item Energy efficiency during test time if you use lp weights, sacrifice accuracy for power saving
        \item Examples and related concepts: dropout, ReLu (sparse gradient),
        influence of feature discretization, 
    \end{itemize}


\subsubsection*{Binarization Schemes}
    \begin{itemize}
        \item Deterministic vs. Stochastic
    \end{itemize}


\subsubsection*{Gradient Estimation}
    \begin{itemize}
        \item Path Derivative Gradient Estimators (reparametrization, ST,
        SlopeAnnealing)
        \item Score Function based Gradient Estimators (reinforce+many others)
        \item Expectation Backpropagation
    \end{itemize}
    See Gumble Softmax Paper for overview

\subsubsection*{Algorithms Adam + BN}
    \begin{itemize}
        \item General Review
        \item Illustrate their importance for QNNs
        \item Implement algorithm to see how it works without Adam and BN?
    \end{itemize}

\subsubsection*{Full Algorithm}
    \begin{itemize}
        \item Try to visualize, illustrate procedure (highlight similarities and
        differences to conventional NN training and inference)
    \end{itemize}

\subsubsection*{Experiments}
    \begin{itemize}
        \item Describe different NN architectures (DNN, CNN, RNN)
        \item Describe data sets (MNIST, CIFAR, ImageNet, PennTree)
        \item binary vs low precision
    \end{itemize}

\subsubsection*{Results}
    \begin{itemize}
        \item Training results
        \item Performance results (look up older benchmarks to compare loss in
        in accuracy to)
        \item find comparisons from other references
    \end{itemize}

\pagebreak

\section*{Evolution of the QNN paper}

\subsubsection*{Training NN with lp multiplications}
    \begin{itemize}
    \item The inital motivation was to reduce the power consumption of multipliers,
    which are claimed to be the most power hungry.
    \item No Binarization of anything yet, only lp weights during forward-prop
    \item A common pattern already shows: lp multipliers + hp accumulators.
    Parameter updates are also hp.
    \item Cost of multiplier $O(precision^2)$ while cost of accumulator $O(precision)$
    \item Problem with fixed point: low dynamic range $\rightarrow$ dynamic fixed point.
    \item Evaluation of maxout on MNIST, CIFAR, SVHN with three formats (floating
    point, fixed point, dynamic fixed point)
    \item Techniques: Maxout, dropout, momentum, weight decay, dyn. fixed point,
    update prc vs prop prec.
    \item Comparison floating point vs. fixed point ?
    \item Explain demand for sufficient precison of update (compared to Propagations)
    due to SGD updates
    \item Half-precision has little to no impact (hp fine-tuning?)
    \item Couple of plots showing final test error as function of everything
    \item References to lp network training in the 90ies!
    \item No evaluation of computational gains, only show how robust NNs are to lp muls.
    \end{itemize}

\subsubsection*{Binary Connect: Training DNNs with binary weights during propagations}
    \begin{itemize}
    \item Emphasis shifts from efficiency to generalization properties (ala dropout)
    \item Only binarization of weights, activations not yet binary! Backprop still contains muls!
    \item mention discretization as form of noise which preserves expected value of weights.
    \item Again: need for hp accumulation (cite several studies incl haml limtied
    precision one), Interesting: reference which says brain synapse precision is
    around 6-12 bits
    \item Relations to dropout, dropconnect (!), variational weight noise?
    \item DropConnect: Only expected value of weights needs hp
    \item Another reference about hardware cost of add+mul [22]
    \item Binarization: Deterministic vs. stochastic, Hard sigmoid more efficient!
    \item Algorithm: Biases not binarized!
    \item Tricks: Weight clipping, batch norm, adam, normalized initialization,
    L2-SVM output layer
    \item Inference: Three ways: keep binary weights, use hp weights or ensemble of
    binary networks (sampled)
    \item Compute dropout+DNN Performance numbers for CIFAR + SVHN?
    \item Fig 1: feature coadaption comparison
    \item Ref. [39] about binary DNN: Fixed-Point feedforward deep Neural...retrains network!
    \item 2/3 of all muls are due to forward/backward prop
    \item No demonstration of power saving/efficieny gains
    \end{itemize}

\subsubsection*{Neural Networks with few Multiplications}
    \begin{itemize}
    \item Muls again in foucs (``Multiplier light networks'')
    \item Binarization of weights + quantization (!) of activations (but only in backprop),
    activations in forward phase are not quantized!
    \item No quantization of BN or ADAM
    \item Convert muls in backprop to bitshifts by pow2 quantization of activations
    \item Ref backprop without muls 1999
    \item First mention of random number generation cost
    \item Ternary connect -1,0,+1, sampling scheme similar to lp reference!
    \item how does hp error signal back prop? how about hp input?
    \item Most muls in weight updates (2MN+3M muls at least)
    \item Mention muls incurred by batch norm! Cost of divison!
    \item Table 2 analysis of total num of muls
    \item Compares only to ordinary SGD (but ok they use the same for the QNN)
    \item QNN converges slower but better in the end fig 1
    \item They dont explain the bit shift operations properly
    \item non-uniform distribution of activations
    \item Explanation of regularization by lp weights - VERY similar to entropy
    SDG ideas! large-basin solution, small description length, Additional reference
    (Neelakatan: Adding gradient noise)
    \end{itemize}

\subsubsection*{Quantized Neural Networks}

    \begin{itemize}
        \item Very lp activations + 6bit gradients (enabling bit-wise ops)
        \item first to actually consider gradient of det./stoch. binary units.
        \item first experiments on RNNs + ImageNet
        \item Extensive references (e.g. network compression for inference)
        \item MAC ops replaced by XNOR and popcount ops
        \item CNNs benefit mainly from activation quantization (large neuron to
        weight ratio)
        \item Binary kernel repetition (worth mentioning?)
        \item Abandon stochastic binarization (only in activations at train time)
        \item NEW: Gradient estimators (clipped straight through) due to binary
        activations. But: weights are binarized as well, but still they take
        gradients w.r.t. weights as if they were continuous...
        \item Shaky argument about independence of binarizations for the
        ``derivation'' of the clipped ST estimator. But intuitively ok (eq. 6)
        \item Address cost of BN, which is particularly large for CNNs + inverse sqrt
        \item How does the approximate shift work in case of negative argument?
        \item Make screenshpt of ``very unoptimized'' baseline gpu kernel (theano repo),
        shift based BN+ADAM are in Torch repo
        \item Algorithm 1: input binarization? activation functions missing - no,
        they use det./stoch. binary units!
        \item Need to disentangle back-prop equations...
        \item What about multiplications in BackBatchNorm? No mention.
        \item Input handling: 8bit fixed point , check algorithm 4...
        \item More than one bit: Quantization+XNORpopcount, Ref to DoReFa net and
        logarithmic data representation papers.
        \item Extensions:
        \begin{itemize}
            \item Approximate BackBatchNorm as well (if not already done)
            \item Use non-uniform quantization scheme for weights/activations
            \item consider a gradient estimate for quantized weights and maybe
            a better estimate than the straight through for activations.
            \item investigate degradation depending on which layers are quantized.
            Are early layers more sensitive? Where can one quantize more aggressively?
        \end{itemize}
    \end{itemize}


\subsubsection*{ZipML Framework}
\begin{itemize}
    \item They apply (non-uniform) quantization scheme only to input data (?)
    \item How would you apply such an adaptive scheme to weights, activations, gradients?
\end{itemize}

\subsubsection*{XNOR-Net}
\begin{itemize}
    \item Their comparison to BNN on ImageNet is really strange, did they implement it
    badly? Because back then BNN did not yet have numbers for imagenet, but they claim
    it performs worse than BNN. Only later Bengio published (better!) numbers than XNOR.
    \item Memory/op count for AlexNet (compare DNN and CNN weight/activation counts)
    \item They consider binary weights+activations and only binary weights.
    \item Mention several network compression approaches (on pretrained networks)
    \item Quite extensive review of related techiques.
    \item Interesting equivalence of sign(W) to minimum square distance to W
    \item Procedure for activation binarization not quite as elegant (quite heuristic).
    \item Influence of pooling on binarized input, use of relu after binary conv.
    \item They actually quantize the gradient as well, but with max scaling - drop
    in accuracy only 1.4\%...suspicious.
    \item Very similar to QNN, only difference is the scaling factor
    \item Speed-up calculations (62x in theory)
\end{itemize}

\subsubsection*{DoReFa-Net}
\begin{itemize}
    \item Gradients in BNN have hp, backprop convolutions consume much time, but actually
    they only sum hp numbers, no muls. DoReFa-Net tries to replace those with bit conv
    kernels. So BNN comes from the mul side, but DoReFa-Net also complains about
    hp adds...
    \item Introduces low bit weights, activations and gradients.
    \item Why does XNOR gradient binarization not count as binary?
    \item Configuration-Space Exploration, precision matters most in gradients,
    activations and weights (in this order). (W,A,G) ca (1,2,4)bits
    \item Gradients need to be stochastically quantized + larger value range.
    \item They address the need to use the straight through estimator for quantization,
    however, they simply use the identity constrained to [0,1]. Is there something
    better for quantize\_k?
    \item Discussion of straight-through estimators for BNN and XNOR
    \item XNOR-Net: Scaling factors make bit-convs impossible for weights*grads (?)
    \item They fail to reproduce results of XNOR-Net for binary neurons! XNOR reports
    severe degradation for BNN...what is wrong there?
    \item They report strong dependence of results on activation function
    \item Funny gradient quantization, mapping to [0,1] plus artificial noise which
    they say is critical. + each image in batch has it own scaling factor
    \item First+last layer weights not quantized! (larger degradation)
    \item Question: Can you get good performance with just low precsion but 
    without all the hocus pocus?
    \item They recgnize need to explicitly take into account the gradient estimate
    for the weight binarization!
    \item How do you backprop through pooling?
    \item Number of channels affects robustness to quantization (table 1). more 
    complex models are more robust.
    \item They have problems to close gap between lp and hp models, they need to
    use hp initialization for lp models.
    \item NO efficiency evaluation
\end{itemize}

\subsubsection*{DropConnect}
\begin{itemize}
    \item Similar to Dropout, but they put connections to zero
    \item Derive bounds on generalization performance
    \item Regularization: weight decay, Bayesian, early stopping, dropout, weight
    elimination, mention more? maxnorm, 
    \item Why is Dropconnect (and dropout) only suitable for fully conn? ok less
    parameters in convs, but actually dropout claims positive effects as well
    \item Quite intensive: different mask for each sample
    \item Dropout does r*a(Wx) but they do a(r*Wx) which introduces problems with
    proper sampling at test time
    \item Some formal insights to dropout
    \item Still it seems that they did not properly read dropout (FC issue, averaging
    at test time)
\end{itemize}

\subsubsection*{Outline Talk}
\begin{itemize}
    \item Motivation: Power wall, limitations, Horowitz, Sejnowski, why turn to
    spezialized hardware, no free lunch theorem? Potential computational savings
    by reduced precision
    \item Show that quantization leads to less degradation than one might expect.
    In constrast: it can even improve generalization under certain circumstances.
    Make analogy to entropy-SGD.
    \item Analysis of forward+backward propagation ops: Identify bottlenecks and
    demonstrate quantization of ops step by step.
    \item Illustrate concepts like Straight-Through-Estimator
    \item Explore configuration space: which parts are sensitive to quantization?
    Provide some intuition (from experiments). NN architecture, dataset, optimization, etc.
    \item Tricks: Show what tricks are used. Which are really necessary?
    \item Hidden ops \& performance holes: Point out where people ignore bottlenecks
    and where they introduce new ones.
\end{itemize}

\subsubsection*{Questions}
\begin{itemize}

    \item what is good/noteworthy about QNNs?
    \item Where do the most significant benefits (computation and accuracy) come from (which aspects)?
    \item what are the major problems with QNNs theoretically/practically?
    \item what do we need to solve before QNNs can be fully employed?
    \item how could one extend/improve QNNs?
    \item How should I go about showing computational gains? All these papers pretty
    much avoid to show it explicitly.
    \item How does the <<>> shift thing work?
    \item ZipML applies quantization scheme only to input?
    \item XNOR-Net...how trustworthy?
\end{itemize}

\subsubsection*{}
\begin{itemize}
    \item 
\end{itemize}

\end{document}