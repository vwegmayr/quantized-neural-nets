\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\begin{document}

\pagenumbering{gobble}

\subsubsection*{Motivation}
    \begin{itemize}
        \item Computational Efficiency (inkl. Horowitz) !! Why do we do this?
        \item Improved Generalization (train with low precision (lp) but keep hp weights and use them during test -> better results) Some power gains during training, no power gains during test.
        \item Energy efficiency during test time if you use lp weights, sacrifice accuracy for power saving
        \item Examples and related concepts: dropout, ReLu (sparse gradient),
        influence of feature discretization, 
    \end{itemize}


\subsubsection*{Binarization Schemes}
    \begin{itemize}
        \item Deterministic vs. Stochastic
    \end{itemize}


\subsubsection*{Gradient Estimation}
    \begin{itemize}
        \item Path Derivative Gradient Estimators (reparametrization, ST,
        SlopeAnnealing)
        \item Score Function based Gradient Estimators (reinforce+many others)
        \item Expectation Backpropagation
    \end{itemize}
    See Gumble Softmax Paper for overview

\subsubsection*{Algorithms Adam + BN}
    \begin{itemize}
        \item General Review
        \item Illustrate their importance for QNNs
        \item Implement algorithm to see how it works without Adam and BN?
    \end{itemize}

\subsubsection*{Full Algorithm}
    \begin{itemize}
        \item Try to visualize, illustrate procedure (highlight similarities and
        differences to conventional NN training and inference)
    \end{itemize}

\subsubsection*{Experiments}
    \begin{itemize}
        \item Describe different NN architectures (DNN, CNN, RNN)
        \item Describe data sets (MNIST, CIFAR, ImageNet, PennTree)
        \item binary vs low precision
    \end{itemize}

\subsubsection*{Results}
    \begin{itemize}
        \item Training results
        \item Performance results (look up older benchmarks to compare loss in
        in accuracy to)
        \item find comparisons from other references
    \end{itemize}

\pagebreak

\section*{Evolution of the QNN paper}

\subsubsection*{Training NN with lp multiplications}
    \begin{itemize}
    \item The inital motivation was to reduce the power consumption of multipliers, which are claimed to be the most power hungry.
    \item A common pattern already shows: lp multipliers + hp accumulators. Parameter updates are also hp.
    \item Cost of multiplier $O(precision^2)$ while cost of accumulator $O(precision)$
    \item Problem with fixed point: low dynamic range $\rightarrow$ dynamic fixed point.
    \item Evaluation of maxout on MNIST, CIFAR, SVHN with three formats (floating
    point, fixed point, dynamic fixed point)
    \item Techniques: Maxout, dropout, momentum, weight decay, dyn. fixed point, update prc vs prop prec.
    \item Comparison floating point vs. fixed point ?
    \item Explain demand for sufficient precison of update (compared to Propagations) due to SGD updates
    \item Half-precision has little to no impact (hp fine-tuning?)
    \item Couple of plots showing final test error as function of everything
    \item References to lp network training in the 90ies!
    \item No evaluation of computational gains, only show how robust NNs are to lp muls.
    \end{itemize}

\subsubsection*{Binary Connect: Training DNNs with binary weights during propagations}
\begin{itemize}
\item Emphasis shifts from 
\end{itemize}

\end{document}