\documentclass{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

\begin{document}

\pagenumbering{gobble}

\subsubsection*{Motivation}
    \begin{itemize}
        \item Computational Efficiency (inkl. Horowitz) !! Why do we do this?
        \item Improved Generalization (train with low precision (lp) but keep hp weights and use them during test -> better results) Some power gains during training, no power gains during test.
        \item Energy efficiency during test time if you use lp weights, sacrifice accuracy for power saving
        \item Examples and related concepts: dropout, ReLu (sparse gradient),
        influence of feature discretization, 
    \end{itemize}


\subsubsection*{Binarization Schemes}
    \begin{itemize}
        \item Deterministic vs. Stochastic
    \end{itemize}


\subsubsection*{Gradient Estimation}
    \begin{itemize}
        \item Path Derivative Gradient Estimators (reparametrization, ST,
        SlopeAnnealing)
        \item Score Function based Gradient Estimators (reinforce+many others)
        \item Expectation Backpropagation
    \end{itemize}
    See Gumble Softmax Paper for overview

\subsubsection*{Algorithms Adam + BN}
    \begin{itemize}
        \item General Review
        \item Illustrate their importance for QNNs
        \item Implement algorithm to see how it works without Adam and BN?
    \end{itemize}

\subsubsection*{Full Algorithm}
    \begin{itemize}
        \item Try to visualize, illustrate procedure (highlight similarities and
        differences to conventional NN training and inference)
    \end{itemize}

\subsubsection*{Experiments}
    \begin{itemize}
        \item Describe different NN architectures (DNN, CNN, RNN)
        \item Describe data sets (MNIST, CIFAR, ImageNet, PennTree)
        \item binary vs low precision
    \end{itemize}

\subsubsection*{Results}
    \begin{itemize}
        \item Training results
        \item Performance results (look up older benchmarks to compare loss in
        in accuracy to)
        \item find comparisons from other references
    \end{itemize}

\pagebreak

\section*{Evolution of the QNN paper}

\subsubsection*{Training NN with lp multiplications}
    \begin{itemize}
    \item The inital motivation was to reduce the power consumption of multipliers, which are claimed to be the most power hungry.
    \item A common pattern already shows: lp multipliers + hp accumulators. Parameter updates are also hp.
    \item Problem with fixed point: low dynamic range $\rightarrow$ dynamic fixed point.
    \item Evaluation of maxout on MNIST, CIFAR, SVHN
    \end{itemize}

\end{document}